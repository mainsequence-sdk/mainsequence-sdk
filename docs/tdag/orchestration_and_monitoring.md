# Orchestration & Monitoring

Once your time series pipelines are built, TDAG offers multiple modes for executing and monitoring their updates efficiently. These modes support local development, debugging, and scalable production deployments.

## Execution Modes

1. **Local Mode** We can run our pipeline locally using local parquet files without interacting with the remote database. This mode is ideal for fast prototyping or parameter sweeps (e.g., hyperparameter tuning). It is faster then the other modes as it does not perform costly database writes.
2. **Debug Mode**: We run our pipelines for one-loop as a single process, persisting and reading from our remote database. This is helpful for debugging and development before moving to production.
3. **Live Mode**: We run our pipelines as a separate distributed process via a Ray cluster. This mode is designed for production use.

## Running Time Series in Local Mode

For quick local development and testing of a new time series we can use the local data lake mode to run the time serie using
```
time_series = CryptoPortfolioTimeSerie()
result = time_series.run_local_update()
```

A classic use-case is to see how a strategy performs with different parameters  by running it in a loop. Here we have a Long Short portfolio and we want to observe the hyperspace of portfolios generated by several combinations of parameters. In this case, we don’t want to persist any iteration in the database; perhaps we just want to see at which point the interaction of the regularization parameters starts to decrease, for example, or at which point our regression starts to stabilize. For these scenarios, we can run our pipelines in **Local Mode** mode.

Let’s look at a code example to understand it better.

```python
total_return = []
for rolling_window in range(60, 30 * 24, 20):
    for lasso_alpha in [1, 1e-2, 1e-3, 1e-4, 1e-5]:
        long_short_portfolio = LongShortPortfolio(
           ticker_long="XLF", 
           ticker_short="XLE",
           long_rolling_windows=[long_rollling_window],
           short_rolling_windows=[100, 200], 
           lasso_alpha=1e-2
        )
        portfolio_df = long_short_portfolio.run_local_update()
        total_return.append(long_short_portfolio["portfolio"].iloc[-1] - 1)

```

If we run in Local Mode, each of the TimeSeries will be dumped once from the database into a Data Lake as a Parquet file. The Data Lake will be configured in a folder structure of the following form:

``` 
DataLakeName/
├── DateRange/
│   ├── TimeSeriesHash1/parquet_partitions
│   ├── TimeSeriesHash2/parquet_partitions
│   ├── TimeSeriesHash3/parquet_partitions
│   └── ...
└── ...
```

## Running Time Series in Live/Debug mode
When we want to move our time series to production, we can execute backend system so it can be distributed and the data stored in the shared database for reusability. 
This is done using the .run() method. 
```
time_series = CryptoPortfolioTimeSerie()
time_series.run(debug_mode=False)
```

We can use additional parameters to specify how the timeseries should run.

- ```debug_mode```: A boolean Setting this to **True** runs the Pipeline in Debug Mode, otherwise in Live Mode.
- ```update_tree```: A boolean variable whether to update all the dependencies of the time series or only the called time series. This is helpful if this time series has many dependencies and we are only interested in the final time serie.
- ```update_only_tree```: A boolean variable whether to update only the dependencies of the time series.
- ```remote_scheduler```: An optional custom scheduler to run the time series. If no remote_scheduler is provided, a default scheduler is created automatically.
- ```force_update```: A scheduler manages at which times to run the time series. This boolean variable is used to ignore the scheduler.

For example, to run this time series immediately in debug mode and only update the called time series, we can use:
```
time_series = CryptoPortfolioTimeSerie()
time_series.run(debug_mode=True, update_tree=False, force_update=True)
```

[//]: # (## Schedulers and how they work)

[//]: # (The ```Schedulers``` object is in charge of organizing and coordinating the update of time series while multiple pipelines are running. )

[//]: # ()
[//]: # (The scheduler that is in charge of updating all our pipeliens which uses an environment of scikit-learn, while we can have another scheduler that updates pipelines on an environment that requires heavy machine learning libraries like Rapids and Pytorch in this case we will have somthing like this.)

[//]: # ()
[//]: # (```mermaid)

[//]: # (graph TD)

[//]: # (    A[Scheduler: For scikit learn environment] --> B[TimeSerie: Type 1])

[//]: # (    A --> E[TimeSerie: Type 2])

[//]: # (    )
[//]: # (    E --> E1[Intermediate Time Series])

[//]: # (    B --> B1[Intermediate Time Series])

[//]: # (    )
[//]: # (    E1 -->P[TimeSeries:Prices])

[//]: # (    B1 -->P)

[//]: # (    )
[//]: # (    A_P[Scheduler: For Nvidia environment] --> B_P[TimeSerie: Type 2])

[//]: # (    A_P --> E_P[TimeSerie: Type 2])

[//]: # (    )
[//]: # (    E_P --> E1_P[Intermediate Time Series])

[//]: # (    B_P --> B1_P[Intermediate Time Series])

[//]: # (    )
[//]: # (    E1_P -->P)

[//]: # (    B1_P -->P   )

[//]: # (    )
[//]: # (    )
[//]: # (    )
[//]: # (    classDef purple fill:#DDA0DD,stroke:#000,stroke-width:2px;)

[//]: # (    class A purple;)

[//]: # (    class A_P purple;)

[//]: # (    )
[//]: # (```)

[//]: # ()
[//]: # (As you can see in the example above, schedulers can have different configurations and mixed time series dependencies. If)

[//]: # (a time series is dependent on a particular environment, you can isolate it and allow a specific scheduler to update it)

[//]: # (only there. You can also let a time series be updated by the next available scheduler. You don't need to worry; this all)

[//]: # (happens in the background. As you will see, setting it up is quite simple.)

[//]: # ()
[//]: # ()
[//]: # (### Scheduler in live mode)

[//]: # ()
[//]: # (While the scheduler in debug mode is handy for testing our pipelines, we want to harness the full power of distributed)

[//]: # (and auto-scalable systems to manage all our pipelines, regardless of their complexity. Let's go back to our original)

[//]: # (example of the ETF replicator and assume that when we update the prices, we can take advantage of parallelization.)

[//]: # ()
[//]: # (Imagine you are requesting prices from an API, allowing you to make batch requests using several cores instead of)

[//]: # (relying on a single loop. In this case, you may want to set the `TimeSerie` `BarPrices` to use perhaps 10 CPUs, while)

[//]: # (the other time series might use only 1 CPU. Alternatively, you might want one time series to use 10 CPUs and 1 GPU.)

[//]: # ()
[//]: # (This and any other configuration is possible thanks to **TDAG**, leveraging Ray as our cluster manager. By using Ray, we)

[//]: # (can easily distribute and parallelize our pipelines and set the requirements we need for each `TimeSerie`.)

[//]: # ()
[//]: # (Going back to our previous example the difference between **live** and **debug** mode can be observed in the same time)

[//]: # (serie)

[//]: # (running differently)

[//]: # ()
[//]: # (#### Scheduler in live mode graph)

[//]: # ()
[//]: # (```mermaid)

[//]: # (graph TD)

[//]: # (    A[TimeSerie: XLF Returns] --> B[TimeSerie: XLF Lasso Regression: 100 Days])

[//]: # (    A --> E[TimeSerie: XLF Lasso Regression: 200 Days])

[//]: # (    )
[//]: # (    A[TimeSerie: XLE Returns] --> B_1[TimeSerie: XLE Lasso Regression: 100 Days])

[//]: # (    A --> E_1[TimeSerie: XLE Lasso Regression: 200 Days])

[//]: # (    )
[//]: # (    B --> |to Timescale DB| D[DB Table for: XLF TimeSerie: Lasso Regression: 100 Days])

[//]: # (    E --> |to Timescale DB| G[DB Table for: XLF TimeSerie: Lasso Regression: 200 Days])

[//]: # (    )
[//]: # (    B_1 --> |to Timescale DB| D_1[DB Table for: XLE TimeSerie: Lasso Regression: 100 Days])

[//]: # (    E_1 --> |to Timescale DB| G_1[DB Table for: XLE TimeSerie: Lasso Regression: 200 Days])

[//]: # (    )
[//]: # (    D --> |to Portfolio TS| last[TimeSerie: Long Short Portfolio])

[//]: # (    G --> |to Portfolio TS| last)

[//]: # (    )
[//]: # (    D_1 --> |to Portfolio TS| last)

[//]: # (    G_1 --> |to Portfolio TS| last)

[//]: # (    )
[//]: # (    classDef purple fill:#DDA0DD,stroke:#000,stroke-width:2px;)

[//]: # (    class D purple;)

[//]: # (    class G purple;)

[//]: # (    class D_1 purple;)

[//]: # (    class G_1 purple;)

[//]: # (```)

[//]: # ()
[//]: # (#### Scheduler in debug mode graph)

[//]: # ()
[//]: # (```mermaid)

[//]: # (graph TD)

[//]: # (    A[TimeSerie: XLF Returns] --> B[TimeSerie: XLF Lasso Regression: 100 Days])

[//]: # (    B --> C[DB Table for: XLF TimeSerie: Lasso Regression: 100 Days])

[//]: # (    C --> D[TimeSerie: XLF Lasso Regression: 200 Days])

[//]: # (    D --> E[DB Table for: XLF TimeSerie: Lasso Regression: 200 Days])

[//]: # (    )
[//]: # (    )
[//]: # (    E--> H[TimeSerie: XLE Lasso Regression: 100 Days])

[//]: # (    H --> I[DB Table for: XLE TimeSerie: Lasso Regression: 100 Days])

[//]: # (    I --> J[TimeSerie: XLE Lasso Regression: 200 Days])

[//]: # (    J --> K[DB Table for: XLE TimeSerie: Lasso Regression: 200 Days])

[//]: # (    K --> L[TimeSerie: Long Short Portfolio])

[//]: # ()
[//]: # (    classDef purple fill:#DDA0DD,stroke:#000,stroke-width:2px;)

[//]: # (    class C,E,I,K purple;)

[//]: # ()
[//]: # (```)

[//]: # ()
[//]: # ()
[//]: # (Additionally, if we want, we can keep some TimeSeries cached in memory that are constantly being reused. This increases)

[//]: # (the speed at which we can run our pipelines by approximately 100 times, as we read mostly from memory or disk, avoiding)

[//]: # (networking and database overhead.)

[//]: # ()
[//]: # (To run a TimeSeries in Data Lake mode, we only need to call the method `get_df_greater_than_in_table` of the `TimeSerie`)

[//]: # (we want to calculate, and the tree will be updated automatically.)

Now, let’s look at how we can visualize and manage all our pipelines in the GUI. Continue
to [Part 3: The TDAG Explorer](tdag_explorer_part3.md).
